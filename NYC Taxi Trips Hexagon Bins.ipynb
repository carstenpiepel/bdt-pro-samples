{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3301030f",
   "metadata": {},
   "source": [
    "# Summarize NYC Taxi Trips into Hexagonal Bins\n",
    "This notebook demonstrates how to use BDT's ST functions to summarize approximately \n",
    "1.5 million taxi trips taken in New York City into hexagonal bins.\n",
    "\n",
    "You can download the taxi data from Kaggle [here](https://www.kaggle.com/c/nyc-taxi-trip-duration/data). \n",
    "You will need a Kaggle account to access the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b08af03",
   "metadata": {},
   "source": [
    "## Start Spark\n",
    "The following code will start a BDT-enabled Spark context within this notebook. Instead of submitting jobs to a separate Spark cluster running somewhere else, Spark executes within Pro. This limits the resources used for the job to the resources of the machine Pro is running on but it is very convenient for prototyping and developing BDT workflows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b80ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_esri import spark_start, spark_stop\n",
    "\n",
    "spark_stop()\n",
    "\n",
    "config = {\n",
    "    \"spark.driver.memory\":\"4G\",\n",
    "    \"spark.kryoserializer.buffer.max\":\"2024\",\n",
    "    \"spark.jars\": \"C:\\\\Users\\\\%USERNAME%\\\\bdt3\\\\bdt-3.0.0-3.2.0-2.12-merge-20220329.5.jar\",\n",
    "    \"spark.submit.pyFiles\": \"C:\\\\Users\\\\%USERNAME%\\\\bdt3\\\\bdt-3.0.0+snapshot.merge.20220329.5-py3.9.egg\"\n",
    "}\n",
    "\n",
    "spark = spark_start(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273601e4",
   "metadata": {},
   "source": [
    "Some more bootstrapping to alias the BDT functions and to activate the BDT license. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "060a85f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import bdt\n",
    "bdt.auth(os.path.expanduser(os.path.join(\"~\", \"bdt3\", \"carsten.lic\")))\n",
    "from bdt import functions as F\n",
    "from bdt import processors as P\n",
    "from bdt import sinks as S\n",
    "from pyspark.sql.functions import rand, col, lit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b7f0c",
   "metadata": {},
   "source": [
    "## Summarize the data using Spark SQL and BDT's spatial functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b7bdc4",
   "metadata": {},
   "source": [
    "Load the taxi data into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1076d386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "load_path = os.path.expanduser(os.path.join(\"~\",\"kaggle\",\"nyc-taxi-trip-duration\",\"train.csv\"))\n",
    "\n",
    "schema = \",\".join([\n",
    "    \"id string\",\n",
    "    \"vendor_id string\",\n",
    "    \"pickup_datetime timestamp\",\n",
    "    \"dropoff_datetime timestamp\",\n",
    "    \"passenger_count integer\",\n",
    "    \"pickup_longitude double\",\n",
    "    \"pickup_latitude double\",\n",
    "    \"dropoff_longitude double\",\n",
    "    \"dropoff_latitude double\",\n",
    "    \"store_and_fwd_flag string\",\n",
    "    \"trip_duration integer\"\n",
    "])\n",
    "\n",
    "df = spark\\\n",
    "    .read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\",True)\\\n",
    "    .option(\"parserLib\", \"univocity\")\\\n",
    "    .option(\"mode\", \"permissive\")\\\n",
    "    .schema(schema)\\\n",
    "    .load(load_path)\\\n",
    "    .drop(\"id\",\"vendor_id\",\"passenger_count\",\"store_and_fwd_flag\")\\\n",
    "    .selectExpr(\"*\",\"hour(pickup_datetime) pickup_hour\")\\\n",
    "    .cache()\n",
    "\n",
    "df.createOrReplaceTempView(\"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bfa5ad",
   "metadata": {},
   "source": [
    "Apply Spark SQL to the data frame. `ST_AsHex()` converts each taxi trip's pickup \n",
    "location into a hexagon with a size of 500 m, `ST_FromHex()` converts the hexagon \n",
    "into a polygonal shape, which is then converted to WKT format using the 'ST_AsText' function.\n",
    "`collect()` retrieves the results from Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f3c5674",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = spark.sql(\"\"\"\n",
    "    SELECT COUNT(1) AS CNT, ST_AsText(ST_FromHex(ST_AsHex(pickup_longitude, pickup_latitude, 500), 500)) AS SHAPE FROM v\n",
    "    GROUP BY ST_AsHex(pickup_longitude, pickup_latitude, 500)\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320619b8",
   "metadata": {},
   "source": [
    "Add the hexagon bins as an in-memory feature class to the current map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15bbcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "\n",
    "ws = \"memory\"\n",
    "nm = \"HexBins\"\n",
    "\n",
    "fc = os.path.join(ws,nm)\n",
    "\n",
    "arcpy.management.Delete(fc)\n",
    "\n",
    "sp_ref = arcpy.SpatialReference(3857)\n",
    "arcpy.management.CreateFeatureclass(ws,nm,\"POLYGON\",spatial_reference=sp_ref)\n",
    "arcpy.management.AddField(fc, \"CNT\", \"LONG\")\n",
    "\n",
    "# Note shape is expected to be in WKT\n",
    "with arcpy.da.InsertCursor(fc, [\"CNT\", \"SHAPE@WKT\"]) as cursor:\n",
    "    for row in rows:\n",
    "        cursor.insertRow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1940c",
   "metadata": {},
   "source": [
    "![](media/taxi_hexagons.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
